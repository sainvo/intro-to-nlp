{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can wget this notebook from https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/bow_classifier.ipynb\n",
    "\n",
    "# Text classification\n",
    "\n",
    "## Objectives of this lecture\n",
    "\n",
    "* Introduce text classification as a task\n",
    "* Introduce basic approaches to text classification, especially the bag-of-words model\n",
    "* Show practical implementation of the concepts in the previous two lectures (intro and machine learning primer)\n",
    "* Train a simple classifier on the IMDB dataset\n",
    "\n",
    "## Purporse of these materials\n",
    "\n",
    "* These materials support, but do not replace the lectures\n",
    "* Presence on lectures highly recommended\n",
    "* Make notes!\n",
    "\n",
    "## Text classification\n",
    "\n",
    "* Text in, class label out\n",
    "* Often thought especially in terms of \"Document classification\" --- document in, class label(s) out\n",
    "  * Movie review -> star rating\n",
    "  * Email -> ham or spam\n",
    "  * News article -> list of topics from a pre-defined set of topic categories\n",
    "  * Comment in online discussion -> abusive or not\n",
    "  * Customer email / call to customer service -> topic and relevant business process\n",
    "  * Customer feedback -> needs reaction or not\n",
    "  * ...\n",
    "  \n",
    " * Document classification can be seen as a supervised classification problem with pre-defined classes\n",
    " * A direct application of the standard approach:\n",
    "   1. prepare training/dev/test data\n",
    "   2. extract features\n",
    "   3. train your favorite classifier\n",
    "   4. test on held-out data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model\n",
    "\n",
    "* BoW is the simplest way to model documents for classification\n",
    "* The document is reduced to a **set** of features, in the simplest case the words\n",
    "\n",
    "\n",
    "* Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example\n",
    "* Values: 1/0 (present/absent) or oftentimes TF/IDF weights (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB data\n",
    "\n",
    "* Movie review sentiment positive/negative\n",
    "* Some 25,000 examples, 50:50 split of classes (why is this number relevant?)\n",
    "* Current state-of-the-art is about 95% accuracy (what is accuracy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: neg\n",
      "text: I did not like the idea of the female turtle at all since 1987 we knew the TMNT to be four brothers with their teacher Splinter and their enemies and each one of the four brothers are named after the great artists name like Leonardo , Michelangleo, Raphel and Donatello so Venus here doesn't have any meaning or playing any important part and I believe that the old TMNT series was much more better than that new one which contains Venus As a female turtle will not add any action to the story we like the story of the TMNT we knew in 1987 to have new enemies in every part is a good point to have some action but to have a female turtle is a very weak point to have some action, we wish to see more new of TMNT series but just as the same characters we knew in 1987 without that female turtle.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"intro-to-nlp/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe! (why?)\n",
    "print(\"class label:\", data[0][\"class\"])\n",
    "print(\"text:\",data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words in practice\n",
    "\n",
    "* We will need to build a feature vector for every example\n",
    "* We will need to know the class for every example\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features), and a value for each feature, 0/1 for binary features, TF-IDF weights are also a typical choice\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This many texts 25000\n",
      "This many labels 25000\n",
      "\n",
      "neg I did not like the idea of the female turtle at al...\n",
      "neg Plot Synopsis: When his wife, a news reporter, is ...\n",
      "pos Yakitate! Ja-pan (translated as Fresh Baked! Japan...\n",
      "neg This is almost like two films--one literate and en...\n",
      "neg My kid makes better videos than this! I feel rippe...\n",
      "pos Why did this movie fail commercially? It's got a s...\n",
      "pos This movie draws you in and gets you hooked on kee...\n",
      "neg ::POTENTIAL SPOILERS::  Man, this movie was awful....\n",
      "neg Although the actors were good, specially Fritzi Ha...\n",
      "neg This movie is like the thousand \\cat and mouse\\\" m...\n",
      "pos This movie is difficult to watch in our fast-paced...\n",
      "neg I'm not usually one to slate a film . I try to see...\n",
      "pos This movie surprised me in a good way. From the bo...\n",
      "pos A first time director (Bromell) has assembled a sm...\n",
      "neg Merry madcaps in London stage a treasure hunt, wit...\n",
      "neg At least the under ten year old set will stay inte...\n",
      "neg Remember the name Kevin Lime - and please, please ...\n",
      "neg I am a huge, huge fan of John Cusack, Samuel L. Ja...\n",
      "pos Pet Sematary is a very good horror film and believ...\n",
      "neg I noticed with some amusement that in the end cred...\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts and labels into separate lists\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(\"This many texts\",len(texts))\n",
    "print(\"This many labels\",len(labels))\n",
    "print()\n",
    "for label,text in list(zip(labels,texts))[:20]:\n",
    "    print(label,text[:50]+\"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn text vectorizers\n",
    "\n",
    "* Vectorizers take care of turning inputs into feature vectors\n",
    "* Also build the feature name to feature index mapping:\n",
    "    * `.fit()` learn the mapping\n",
    "    * `.fit_transform()` learn and apply the mapping\n",
    "    * `.transform()` apply the learned mapping\n",
    "    * (What is the difference?)\n",
    "\n",
    "Let's first try on a tiny example, then work our way to the real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique features:\n",
      "['commodities', 'global', 'gold', 'has', 'is', 'markets', 'metal', 'more', 'of', 'on', 'palladium', 'precious', 'price', 'soared', 'soaring', 'than', 'the', 'why']\n",
      "\n",
      "Feature vectors (sparse format):\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 16)\t3\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#just two short document\n",
    "toy_data=[\"More precious than gold: Why the metal palladium is soaring\",\"The price of the precious metal palladium has soared on the global commodities markets.\"]\n",
    "\n",
    "vectorizer.fit(toy_data)\n",
    "print(\"Unique features:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print()\n",
    "print(\"Feature vectors (sparse format):\")\n",
    "print(vectorizer.transform(toy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1]\n",
      " [1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "#...and in a more understandable format\n",
    "#...these are the feature vectors of our two toy documents\n",
    "print(vectorizer.transform(toy_data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# only features seen in the training data can be taken into account!\n",
    "print(vectorizer.transform([\"unseen words only\"]).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 74849)\n",
      "what did we get? -> <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "print(\"what did we get? ->\", feature_matrix.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18292)\t1\n",
      "  (0, 46050)\t1\n",
      "  (0, 38755)\t1\n",
      "  (0, 66339)\t1\n",
      "  (0, 32435)\t1\n",
      "  (0, 46680)\t1\n",
      "  (0, 24202)\t1\n",
      "  (0, 68640)\t1\n",
      "  (0, 4753)\t1\n",
      "  (0, 2662)\t1\n",
      "  (0, 60351)\t1\n",
      "  (0, 402)\t1\n",
      "  (0, 72365)\t1\n",
      "  (0, 36865)\t1\n",
      "  (0, 67117)\t1\n",
      "  (0, 67125)\t1\n",
      "  (0, 6334)\t1\n",
      "  (0, 25734)\t1\n",
      "  (0, 9304)\t1\n",
      "  (0, 73342)\t1\n",
      "  (0, 66367)\t1\n",
      "  (0, 65747)\t1\n",
      "  (0, 62338)\t1\n",
      "  (0, 3258)\t1\n",
      "  (0, 21827)\t1\n",
      "  :\t:\n",
      "  (24999, 3538)\t1\n",
      "  (24999, 57668)\t1\n",
      "  (24999, 49147)\t1\n",
      "  (24999, 66526)\t1\n",
      "  (24999, 66621)\t1\n",
      "  (24999, 31868)\t1\n",
      "  (24999, 13574)\t1\n",
      "  (24999, 31669)\t1\n",
      "  (24999, 68412)\t1\n",
      "  (24999, 45388)\t1\n",
      "  (24999, 66329)\t1\n",
      "  (24999, 45656)\t1\n",
      "  (24999, 50496)\t1\n",
      "  (24999, 72910)\t1\n",
      "  (24999, 33779)\t1\n",
      "  (24999, 18839)\t1\n",
      "  (24999, 72239)\t1\n",
      "  (24999, 17239)\t1\n",
      "  (24999, 29971)\t1\n",
      "  (24999, 70493)\t1\n",
      "  (24999, 12484)\t1\n",
      "  (24999, 51610)\t1\n",
      "  (24999, 15621)\t1\n",
      "  (24999, 67385)\t1\n",
      "  (24999, 25109)\t1\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '020410', '029', '03', '04', '041', '05', '050', '06', '06th', '07', '08', '087', '089', '08th', '09', '0f', '0ne', '0r', '0s', '10', '100', '1000', '1000000', '10000000000000', '1000lb', '1000s', '1001', '100b', '100k', '100m', '100min', '100mph', '100s', '100th', '100x', '100yards', '101', '101st', '102', '102nd', '103', '104', '1040', '1040a', '1040s', '105', '1050', '105lbs', '106', '106min', '107', '108', '109', '10am', '10lines', '10mil', '10min', '10minutes', '10p', '10pm', '10s', '10star', '10th', '10x', '10yr', '11', '110', '1100', '11001001', '1100ad', '111', '112', '1138', '114', '1146', '115', '116', '117', '11f', '11m', '11th', '12', '120', '1200', '1200f', '1201', '1202', '123', '12383499143743701', '125', '125m', '127', '128', '12a', '12hr', '12m', '12mm', '12s', '12th', '13', '130', '1300', '1300s', '131', '1318', '132', '134', '135', '135m', '136', '137', '138', '139', '13k', '13s', '13th', '14', '140', '1408', '140hp', '1415', '142', '145', '1454', '146', '147', '1473', '149', '1492', '14a', '14ieme', '14s', '14th', '14yr', '14ème', '15', '150', '1500', '1500s', '150_worst_cases_of_nepotism', '150k', '150m', '151', '152', '153', '1547', '155', '156', '1561', '157', '158', '1594', '15mins', '15minutes', '15s', '15th', '16', '160', '1600', '1600s', '160lbs', '161', '1610', '163', '164', '165', '166', '1660s', '168', '169', '1692', '16ieme', '16k', '16mm', '16s', '16th', '16x9', '16ème', '16éme', '17', '170', '1700', '1700s', '1701', '171', '175', '177', '1775', '1780s', '1790s', '1794', '1798', '17million', '17th', '18', '180', '1800', '1800mph', '1800s', '1801', '1805', '1809', '180d', '1812', '1813', '18137', '1814', '1816', '1820', '1824', '183', '1830', '1832', '1836', '1837', '1838', '1839', '1840', '1840s', '1844', '1846', '1847', '185', '1850', '1850ies', '1850s', '1852', '1853', '1854', '1855', '1859', '1860', '1860s', '1861', '1862', '1863', '1864', '1865', '1870', '1870s', '1871', '1873', '1874', '1875', '1876', '188', '1880', '1880s', '1881', '1886', '1887', '1888', '1889', '188o', '1890', '1890s', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '18a', '18s', '18th', '18year', '19', '190', '1900', '1900s', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1910s', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192', '1920', '1920ies', '1920s', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1930ies', '1930s', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '193o', '194', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1949er', '195', '1950', '1950s', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1960s', '1961', '1961s', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '197', '1970', '1970ies', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '19796', '197o', '1980', '1980ies', '1980s', '1981', '1982', '1982s', '1983', '1983s', '1984', '1984ish', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19k', '19th', '19thc', '1am', '1and', '1d', '1h', '1h30', '1h40', '1h40m', '1h53', '1hour', '1hr', '1million', '1min', '1mln', '1o', '1s', '1st', '1ton', '1tv', '1½', '1ç', '20', '200', '2000', '20000', '20001', '2000ad', '2000s', '2001', '2002', '2003', '2004', '2004s', '2005', '2006', '2007', '2008', '2009', '200ft', '200th', '201', '2010', '2012', '2013', '2015', '2017', '2019', '2020', '2022', '2023', '2030', '2031', '2033', '2035', '2036', '2038', '204', '2040', '2044', '2046', '2047', '2050', '2053', '2054', '206', '2060', '2070', '2080', '209', '2090', '20c', '20ft', '20k', '20m', '20mins', '20minutes', '20mn', '20p', '20perr', '20s', '20th', '20ties', '20widow', '20x', '20year', '20yrs', '21', '210', '2100', '214', '215', '2151', '216', '21699', '21849889', '21849890', '21849907', '21st', '22', '220', '2200', '221', '2210', '22101', '222', '223', '225', '2257', '225mins', '227', '22d', '22h45', '22nd', '23', '230lbs', '230mph', '231', '232', '233', '236', '237', '23d', '23rd', '24', '240', '2400', '241', '242', '248', '2480', '249', '24m30s', '24th', '24years', '25', '250', '2500', '250000', '25million', '25mins', '25s', '25th', '25yo', '25yrs', '26', '260', '2600', '261k', '262', '2642', '269', '26th', '27', '270', '272', '273', '274', '275', '2772', '278', '27th', '27x41', '28', '280', '285', '28th', '29', '29th', '2am', '2d', '2fast', '2furious', '2h', '2h30', '2hour', '2hours', '2hr', '2hrs', '2in', '2inch', '2k', '2more', '2nd', '2oo4', '2oo5', '2pac', '2point4', '2s', '2x4', '30', '300', '3000', '300ad', '300c', '300lbs', '300mln', '3012', '303', '305', '30am', '30ish', '30k', '30lbs', '30min', '30mins', '30pm', '30s', '30something', '30th', '30ties', '31', '3199', '31st', '32', '3200', '320x180', '32lb', '32nd', '33', '330am', '330mins', '332960073452', '336th', '33m', '34', '345', '3462', '34th', '35', '350', '3500', '3516', '356', '357', '35c', '35mins', '35mm', '35pm', '35th', '35yr', '36', '360', '365', '36th', '37', '370', '372', '378', '38', '38k', '38th', '39', '395', '39th', '3am', '3bs', '3d', '3dvd', '3k', '3lbs', '3m', '3mins', '3p', '3p0', '3pm', '3po', '3rd', '3rds', '3th', '3who', '3x5', '3yrs', '40', '400', '4000', '401k', '405', '409', '40am', '40min', '40mins', '40mph', '40s', '40th', '41', '42', '420', '425', '428', '42nd', '43', '430', '44', '440', '442nd', '44c', '44yrs', '45', '450', '4500', '451', '454', '45am', '45min', '45mins', '45s', '46', '465', '469', '47', '475', '477', '47s', '48', '480m', '480p', '48hrs', '49', '498', '49th', '4am', '4cylinder', '4d', '4eva', '4ever', '4f', '4h', '4hrs', '4k', '4kids', '4m', '4o', '4pm', '4th', '4w', '4ward', '4x', '4x4', '50', '500', '5000', '500000', '500ad', '500db', '500lbs', '502', '50c', '50ft', '50ies', '50ish', '50k', '50min', '50mins', '50s', '50th', '50usd', '51', '51b', '51st', '52', '5200', '5250', '529', '52s', '53', '53m', '54', '5400', '540i', '54th', '55', '5539', '555', '55th', '56', '57', '571', '576', '578', '57d', '58', '58th', '59', '598947', '59th', '5hrs', '5ive', '5kph', '5million', '5min', '5mins', '5s', '5seconds', '5th', '5x', '5x5', '5years', '5yo', '5yrs', '60', '600', '6000', '607', '608', '60ies', '60ish', '60mph', '60s', '60th', '60ties', '61', '618', '62', '6200', '62229249', '63', '637', '63rd', '64', '65', '65m', '66', '660', '666', '66er', '67', '6723', '67th', '68', '68th', '69', '69th', '6am', '6b', '6ft', '6hours', '6k', '6million', '6pm', '6th', '6wks', '6yo', '70', '700', '701', '707', '70ies', '70m', '70mm', '70s', '70th', '71', '713', '72', '72nd', '73', '7300', '735', '737', '74', '740', '740il', '747', '747s', '74th', '75', '750', '75054', '75c', '75m', '76', '7600', '77', '78', '788', '78rpm', '79', '79th', '7days', '7even', '7eventy', '7ft', '7ish', '7mm', '7th', '7½th', '80', '800', '8000', '80ies', '80ish', '80min', '80s', '80yr', '81', '817', '819', '82', '820', '8217', '8230', '83', '84', '849', '84f', '84s', '85', '850', '850pm', '86', '86s', '87', '8700', '8763', '878', '87minutes', '88', '88min', '89', '89or', '89s', '8bit', '8ftdf', '8k', '8mm', '8o', '8p', '8pm', '8star', '8th', '8u', '8½', '90', '900', '9000', '90210', '905', '90c', '90ish', '90min', '90mins', '90s', '91', '911', '914', '917', '92', '921', '92fs', '92nd', '93', '937', '94', '9484', '94s', '94th', '95', '950', '95th', '96', '97', '970', '974th', '978', '98', '987', '98minutes', '99', '998', '999', '9999', '99cents', '99p', '99½', '9_', '9am', '9as', '9do', '9ers', '9is', '9lbs', '9mm']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split\n",
    "\n",
    "* Train data - all training based on it (this includes the vectorizer!)\n",
    "* Development data - set the parameters\n",
    "* Test data - used for nothing during training, produce final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels=train_test_split(texts,labels,test_size=0.2)\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix_train=vectorizer.fit_transform(train_texts)\n",
    "feature_matrix_dev=vectorizer.transform(dev_texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68408)\n",
      "(5000, 68408)\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix_train.shape)\n",
    "print(feature_matrix_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier train\n",
    "\n",
    "* Let us try the venerable, if a bit outdated SVM\n",
    "* Linear SVM for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.0005, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "classifier=sklearn.svm.LinearSVC(C=0.0005,verbose=1)\n",
    "classifier.fit(feature_matrix_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "* For a quick test we can use the .score() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.8654\n",
      "TRAIN 0.89515\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\",classifier.score(feature_matrix_dev, dev_labels))\n",
    "print(\"TRAIN\",classifier.score(feature_matrix_train, train_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try varying the C value and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos' 'pos' ... 'neg' 'neg' 'pos']\n",
      "[[2150  362]\n",
      " [ 311 2177]]\n",
      "0.8654\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "predictions_dev=classifier.predict(feature_matrix_dev)\n",
    "print(predictions_dev)\n",
    "print(sklearn.metrics.confusion_matrix(dev_labels,predictions_dev))\n",
    "print(sklearn.metrics.accuracy_score(dev_labels,predictions_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained classifier save and load\n",
    "\n",
    "* We fitted two things: the vectorizer and the classifier\n",
    "* If we want to reuse them later, we need to save them\n",
    "* You can use `pickle` in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"saved_model.pickle\",\"wb\") as f:\n",
    "    pickle.dump((classifier,vectorizer),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* let's try to load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV - loaded (should match the score above) 0.8654\n"
     ]
    }
   ],
   "source": [
    "with open(\"saved_model.pickle\",\"rb\") as f:\n",
    "    classifier_loaded,vectorizer_loaded=pickle.load(f)\n",
    "\n",
    "feature_matrix_dev_loaded=vectorizer_loaded.transform(dev_texts)\n",
    "print(\"DEV - loaded (should match the score above)\",classifier_loaded.score(feature_matrix_dev, dev_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
